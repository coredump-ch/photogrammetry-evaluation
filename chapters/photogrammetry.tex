\chapter{3D-Rekonstruktion}

\section{Rekonstruktion mittels Photogrammetrie}

Es gibt verschiedene Methoden, um ein Objekt aus der echten Welt in ein
digitales 3D-Modell umzuwandeln. Eine davon ist die Photogrammetrie.

Der Begriff Photogrammetrie beschreibt Methoden, um aus zweidimensionalen
Bildern ein dreidimensionales Objekt zu rekonstruieren.

In unserem Fall möchten wir nicht beim 3D-Modell aufhören, sondern dieses in
ein für 3D-Drucker geeignetes Format bringen. Wir möchten also ein
physikalisches Objekt digitalisieren und anschliessend wieder mit einem
3D-Drucker ausdrucken.

%----------------------------------------------------------------------------------------

\section{Das Bildmaterial}

Die ideale Ausgangsquelle für 3D-Rekonstruktion mittels Photogrammetrie ist
eine hohe Anzahl qualitativ hochwertiger Fotos des Zielobjektes von allen
Seiten. Jede Oberfläche sollte darin zu sehen sein.
%
\marginpar{Bei der Georeferenzierung werden Daten in Bezug zu
geografischen Gegebenheiten gesetzt. So können Skalierungs\-/Faktoren
oder Koordinaten einzelner Datenpunkte im Raum ermittelt werden. Ein
mögliches Beispiel dafür ist ein Luftbild, welches passgenau über eine Karte
gelegt wird.}
%
Die Bilder sollten sich überlappen, damit die Photogrammetrie\-/Software daraus
die ursprüngliche Oberfläche errechnen kann. Idealerweise enthalten die Bilder
auch GNSS-Koordinaten. Damit ist eine einfache Georeferenzierung möglich und
der Rekonstruktions-Prozess kann beschleunigt werden.

%----------------------------------------------------------------------------------------

\section{Feature-Erkennung} \label{photogrammetry:features}

Als nächster Schritt wird das Bildmaterial in eine Photogrammetrie\-/Software
geladen. Diese versucht nun, auf den Bildern sogenannte "<Features"> zu
erkennen. Features (Deutsch: Merkmale) sind mathematisch interessante Stellen
eines Bildes, welche genutzt werden können, um die Dimensionen eines Objektes
unabhängig von dessen Skalierung oder Verzerrung zu extrahieren.

Die Features sind eine wichtige Grundlage für die weitere Verarbeitung im
Rekonstruktions-Prozess.

%----------------------------------------------------------------------------------------

\section{Sparse Point Cloud}

\label{photogrammetry:sparse-point-cloud}

Die dünn besetzte Punktwolke, im Englischen "<Sparse Point Cloud"> genannt,
besteht aus Punkten im 3D-Raum. Sie enthält die nötigsten Punkte aus der
3D-Rekonstruktion und lässt bereits die Konturen des Objektes erahnen. Sie
stellt jedoch noch keine vollständige Rekonstruktion des Objektes dar.

Die Punktwolke wird aus den im Schritt \ref{photogrammetry:features} erkannten
Features generiert. Ein Feature, das in mehreren Bildern erkannt wird, kann als
gemeinsamer Referenzpunkt im 3D-Raum genutzt werden und wird dann zu einem Punkt
in der Punktwolke.

%----------------------------------------------------------------------------------------

\section{Dense Point Cloud}

\label{photogrammetry:dense-point-cloud}

Nachdem die dünn besetzte Punktwolke generiert wurde, kann nun der leere Raum
zwischen den 3D-Punkten mithilfe des existierenden Bildmaterials und den
Referenzpunkten eingefüllt werden. Dieser Prozess nennt sich auch
"<Verdichtung"> oder "<Densification">. Als Ergebnis erhält man eine dicht
besetzte Punktwolke, die ein Vielfaches der Punkte aus der dünn besetzten
Punktwolke enthält. Die Punkte können zudem Metadaten wie beispielsweise
Farbinformationen speichern.

%----------------------------------------------------------------------------------------

\section{Mesh}

\label{photogrammetry:mesh}

Aus der dicht besetzten Punktwolke kann nun ein Objekt mit einer Oberfläche, das
sogenannte "<Mesh">, generiert werden. Ein Mesh, auch Polygonnetz genannt, ist
eine Oberfläche bestehend aus vielen aneinanderhängenden Polygonen, in der Regel
Drei- oder Vierecken.

Die Punktwolke enthält in der Regel zu viele und auch falsche Punkte. Um daraus
ein Mesh zu erzeugen, muss die Punktwolke zuerst bereinigt werden. Dies
geschieht, indem nicht benötigte Punkte gelöscht und dichte Punktgruppen auf
einen Punkt zusammengefasst werden. Diese Rausch-Reduzierung verhilft den
Mesh-Rekon\-struk\-tions-Algorithmen zu besseren Ergebnissen. In unserem Fall wurde
die Bereinigung von Hand durchgeführt.

Zur Umwandlung einer bereinigten Punktwolke in ein Mesh existieren ebenfalls
diverse Algorithmen. Ein besonders gut geeigneter Algorithmus ist der von
Microsoft Research im Jahr 2006 entwickelte "<Poisson Surface
Reconstruction">\cite{kazhdan:2006} Algorithmus.

%----------------------------------------------------------------------------------------

\section{Solid}

Damit ein Mesh druckbar wird, muss es in ein solides Objekt umgewandelt werden.
Ein Mesh besteht nur aus einer Oberfläche, ein solides Objekt hinegegen ist
"<wasserdicht"> und hat ein Volumen. Wichtig bei der Umwandlung ist auch, dass
das Mesh eine Mannigfaltigkeit (Englisch: Manifold) ist. Das bedeutet unter
anderem, dass alle Flächen nach aussen zeigen, dass keine Vertizes oder Kanten
mehrfach vorhanden sind und dass an jeder Kante zwei Flächen anliegen.

Sind diese Voraussetzungen gegeben, kann ein Mesh mit geeigneter Software in
ein Solid-Format wie STL\footnote{STereoLithography,
	\url{https://de.wikipedia.org/wiki/STL-Schnittstelle}} exportiert werden.

%----------------------------------------------------------------------------------------

\section{Slicing}

\label{photogrammetry:slicing}

Damit ein solides Objekt von einem 3D-Drucker gedruckt werden kann, muss es bei
in druckbare Schichten umgewandelt werden. Dieser Prozess nennt sich "<Slicing">
und wird in der Regel von der dem 3D-Drucker mitgelieferten Software erledigt.
%
\marginpar{G-Code ist ein Steuerungscode für computergesteuerte Maschinen wie
CNC-Fräsen oder 3D-Drucker. Er enthält Befehle und Koordinaten um den Druckkopf
zu positionieren und das Material zu extrudieren. Auch die Temperatur, die
Lüfter und weitere Parameter können angesteuert werden.}
%
Das nahezu universell dafür benutzte Datenformat ist der sogenannte
G-Code\footnote{\url{https://en.wikipedia.org/wiki/G-code}}.

Beim Slicing werden ideale Bahnen für den Druckkopf berechnet. Dabei werden
Parameter wie Schichtdicke, Druckkopf-Durchmesser, Geschwindigkeit usw.
berücksichtigt. Die meisten Slicer ermöglichen auch die Generierung von
Stützmaterial oder Haftungshilfen wie einem "<Raft"> oder "<Brim">.

Der resultierende G-Code kann dann direkt an den 3D-Drucker gesendet werden. Der
Rekonstruktions-Workflow ist nun abgeschlossen, das gescannte Objekt entsteht
nun auf der Druckplatte.
